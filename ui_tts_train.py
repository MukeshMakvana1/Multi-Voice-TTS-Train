import gradio as gr
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
import torchaudio
import pandas as pd
import numpy as np
import os
import json
import shutil
from pathlib import Path
from datetime import datetime
import soundfile as sf
from tqdm import tqdm
import random

# Create necessary folders
os.makedirs("voice", exist_ok=True)
os.makedirs("checkpoints", exist_ok=True)
os.makedirs("logs", exist_ok=True)
os.makedirs("datasets", exist_ok=True)
os.makedirs("datasets/wavs", exist_ok=True)

# ==================== MODEL ARCHITECTURE ====================
class TextEncoder(nn.Module):
    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = nn.Parameter(torch.randn(1, 5000, d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
    def forward(self, x):
        x = self.embedding(x)
        x = x + self.pos_encoder[:, :x.size(1), :]
        return self.transformer(x)

class MelDecoder(nn.Module):
    def __init__(self, d_model=512, n_mels=80):
        super().__init__()
        self.prenet = nn.Sequential(
            nn.Linear(d_model, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        
        decoder_layer = nn.TransformerDecoderLayer(d_model, 8, dim_feedforward=2048, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, 6)
        
        self.mel_projection = nn.Linear(d_model, n_mels)
        self.postnet = nn.Sequential(
            nn.Conv1d(n_mels, 512, kernel_size=5, padding=2),
            nn.BatchNorm1d(512),
            nn.Tanh(),
            nn.Dropout(0.5),
            nn.Conv1d(512, 512, kernel_size=5, padding=2),
            nn.BatchNorm1d(512),
            nn.Tanh(),
            nn.Dropout(0.5),
            nn.Conv1d(512, n_mels, kernel_size=5, padding=2),
            nn.BatchNorm1d(n_mels),
        )
        
    def forward(self, memory, mel_input):
        x = self.prenet(mel_input)
        x = self.decoder(x, memory)
        mel_output = self.mel_projection(x)
        
        mel_postnet = self.postnet(mel_output.transpose(1, 2))
        mel_postnet = mel_postnet.transpose(1, 2)
        
        return mel_output, mel_postnet

class VoiceIDEncoder(nn.Module):
    def __init__(self, num_voices, d_model=512):
        super().__init__()
        self.embedding = nn.Embedding(num_voices, d_model)
        
    def forward(self, voice_ids):
        return self.embedding(voice_ids)

class TTSModel(nn.Module):
    def __init__(self, vocab_size, num_voices=100, d_model=512, n_mels=80):
        super().__init__()
        self.text_encoder = TextEncoder(vocab_size, d_model)
        self.voice_encoder = VoiceIDEncoder(num_voices, d_model)
        self.mel_decoder = MelDecoder(d_model, n_mels)
        self.duration_predictor = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 1)
        )
        
    def forward(self, text, voice_ids, mel_target=None):
        text_encoded = self.text_encoder(text)
        voice_emb = self.voice_encoder(voice_ids).unsqueeze(1)
        
        memory = text_encoded + voice_emb
        
        if mel_target is not None:
            mel_input = F.pad(mel_target[:, :-1, :], (0, 0, 1, 0))
        else:
            mel_input = torch.zeros(text.size(0), 1, 80).to(text.device)
        
        durations = self.duration_predictor(text_encoded).squeeze(-1)
        mel_output, mel_postnet = self.mel_decoder(memory, mel_input)
        
        return mel_output, mel_postnet, durations

# ==================== VOCODER ====================
class Vocoder(nn.Module):
    def __init__(self, n_mels=80, upsample_scales=[8, 8, 2, 2]):
        super().__init__()
        self.upsample_scales = upsample_scales
        
        self.conv_pre = nn.Conv1d(n_mels, 512, 7, 1, padding=3)
        
        self.ups = nn.ModuleList()
        self.res_blocks = nn.ModuleList()
        
        channels = [512, 256, 128, 64, 32]
        for i, scale in enumerate(upsample_scales):
            self.ups.append(nn.ConvTranspose1d(channels[i], channels[i+1], 
                                              scale * 2, scale, padding=scale // 2))
            self.res_blocks.append(nn.Sequential(
                nn.Conv1d(channels[i+1], channels[i+1], 3, 1, padding=1),
                nn.LeakyReLU(0.1),
                nn.Conv1d(channels[i+1], channels[i+1], 3, 1, padding=1),
            ))
        
        self.conv_post = nn.Conv1d(32, 1, 7, 1, padding=3)
        
    def forward(self, mel):
        x = self.conv_pre(mel)
        
        for up, res in zip(self.ups, self.res_blocks):
            x = F.leaky_relu(x, 0.1)
            x = up(x)
            x = res(x) + x
        
        x = F.leaky_relu(x, 0.1)
        x = self.conv_post(x)
        x = torch.tanh(x)
        
        return x.squeeze(1)

# ==================== DATASET ====================
class TTSDataset(Dataset):
    def __init__(self, metadata_path, dataset_dir, max_text_len=200, max_mel_len=1000):
        self.dataset_dir = Path(dataset_dir)
        self.max_text_len = max_text_len
        self.max_mel_len = max_mel_len
        
        # Load metadata
        df = pd.read_csv(metadata_path, sep='|', header=None, names=['audio_file', 'text', 'speaker'])
        self.data = df.to_dict('records')
        
        # Build character vocabulary
        all_text = ' '.join(df['text'].tolist())
        self.chars = sorted(list(set(all_text)))
        self.char_to_idx = {c: i+1 for i, c in enumerate(self.chars)}
        self.char_to_idx['<PAD>'] = 0
        self.char_to_idx['<EOS>'] = len(self.chars) + 1
        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}
        
        # Speaker to ID mapping
        speakers = df['speaker'].unique()
        self.speaker_to_id = {speaker: i for i, speaker in enumerate(speakers)}
        self.num_speakers = len(speakers)
        
    def __len__(self):
        return len(self.data)
    
    def text_to_sequence(self, text):
        return [self.char_to_idx.get(c, 0) for c in text] + [self.char_to_idx['<EOS>']]
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Load audio
        audio_path = self.dataset_dir / item['audio_file']
        if not audio_path.exists():
            audio_path = self.dataset_dir / 'wavs' / item['audio_file']
        
        try:
            waveform, sr = torchaudio.load(str(audio_path))
            
            # Resample to 22050 if needed
            if sr != 22050:
                resampler = torchaudio.transforms.Resample(sr, 22050)
                waveform = resampler(waveform)
            
            # Convert to mel spectrogram
            mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=22050,
                n_fft=1024,
                hop_length=256,
                n_mels=80
            )
            mel = mel_transform(waveform).squeeze(0).T
            
            # Text to sequence
            text_seq = self.text_to_sequence(item['text'])
            
            # Speaker ID
            speaker_id = self.speaker_to_id[item['speaker']]
            
            # Padding
            if len(text_seq) > self.max_text_len:
                text_seq = text_seq[:self.max_text_len]
            else:
                text_seq = text_seq + [0] * (self.max_text_len - len(text_seq))
            
            if mel.size(0) > self.max_mel_len:
                mel = mel[:self.max_mel_len, :]
            else:
                padding = torch.zeros(self.max_mel_len - mel.size(0), 80)
                mel = torch.cat([mel, padding], dim=0)
            
            return {
                'text': torch.LongTensor(text_seq),
                'mel': mel,
                'speaker_id': speaker_id,
                'audio_path': str(audio_path)
            }
        except Exception as e:
            print(f"Error loading {audio_path}: {e}")
            # Return dummy data
            return {
                'text': torch.zeros(self.max_text_len, dtype=torch.long),
                'mel': torch.zeros(self.max_mel_len, 80),
                'speaker_id': 0,
                'audio_path': ''
            }

# ==================== TRAINING ====================
class TTSTrainer:
    def __init__(self, model, vocoder, device='cpu'):
        self.model = model.to(device)
        self.vocoder = vocoder.to(device)
        self.device = device
        self.training_history = []
        
    def train(self, dataloader, epochs, learning_rate, save_dir, progress_callback=None):
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = CosineAnnealingLR(optimizer, T_max=epochs)
        
        mel_loss_fn = nn.L1Loss()
        duration_loss_fn = nn.MSELoss()
        
        best_loss = float('inf')
        
        for epoch in range(epochs):
            self.model.train()
            total_loss = 0
            num_batches = len(dataloader)
            
            for batch_idx, batch in enumerate(dataloader):
                text = batch['text'].to(self.device)
                mel = batch['mel'].to(self.device)
                speaker_id = torch.LongTensor(batch['speaker_id']).to(self.device)
                
                optimizer.zero_grad()
                
                mel_output, mel_postnet, durations = self.model(text, speaker_id, mel)
                
                # Calculate losses
                mel_loss = mel_loss_fn(mel_output, mel) + mel_loss_fn(mel_postnet, mel)
                
                # Duration target
                duration_target = torch.ones_like(durations)
                duration_loss = duration_loss_fn(durations, duration_target)
                
                loss = mel_loss + duration_loss * 0.1
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                total_loss += loss.item()
                
                if progress_callback and batch_idx % 5 == 0:
                    progress = (epoch * num_batches + batch_idx) / (epochs * num_batches)
                    progress_callback(progress, f"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{num_batches} | Loss: {loss.item():.4f}")
            
            scheduler.step()
            avg_loss = total_loss / num_batches
            
            self.training_history.append({
                'epoch': epoch + 1,
                'loss': avg_loss,
                'lr': scheduler.get_last_lr()[0]
            })
            
            # Save best model
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_model(save_dir, "best_model.pth")
            
            # Save checkpoint every 10 epochs
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(save_dir, epoch + 1, avg_loss)
            
            if progress_callback:
                progress_callback((epoch + 1) / epochs, f"‚úÖ Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Best: {best_loss:.4f}")
        
        # Save final model
        self.save_model(save_dir, "final_model.pth")
        
        return self.training_history
    
    def save_checkpoint(self, save_dir, epoch, loss):
        checkpoint_path = Path(save_dir) / f"checkpoint_epoch_{epoch}.pth"
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'vocoder_state_dict': self.vocoder.state_dict(),
            'loss': loss
        }, checkpoint_path)
        print(f"üíæ Checkpoint saved: {checkpoint_path}")
    
    def save_model(self, save_dir, filename):
        model_path = Path(save_dir) / filename
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'vocoder_state_dict': self.vocoder.state_dict()
        }, model_path)
        print(f"üíæ Model saved: {model_path}")

# ==================== INFERENCE ====================
def synthesize_speech(model, vocoder, text, speaker_id, char_to_idx, device='cpu'):
    model.eval()
    vocoder.eval()
    
    with torch.no_grad():
        # Convert text to sequence
        text_seq = [char_to_idx.get(c, 0) for c in text] + [char_to_idx.get('<EOS>', 1)]
        text_tensor = torch.LongTensor(text_seq).unsqueeze(0).to(device)
        speaker_tensor = torch.LongTensor([speaker_id]).to(device)
        
        # Generate mel spectrogram
        mel_output, mel_postnet, _ = model(text_tensor, speaker_tensor)
        
        # Convert mel to audio
        audio = vocoder(mel_postnet.transpose(1, 2))
        
        return audio.cpu().numpy()

# ==================== GLOBAL VARIABLES ====================
global_model = None
global_vocoder = None
global_trainer = None
global_dataset = None
global_config = {}

# ==================== GRADIO FUNCTIONS - FIXED ====================

def upload_dataset(metadata_file, audio_files):
    """FIXED: Properly handle Gradio file uploads"""
    try:
        if metadata_file is None:
            return "‚ùå Please upload metadata.csv file"
        
        # Gradio returns file path as string, not bytes
        metadata_path = "datasets/metadata.csv"
        
        # Copy metadata file from temporary location
        shutil.copy(metadata_file, metadata_path)
        
        # Create wavs directory
        wavs_dir = Path("datasets/wavs")
        wavs_dir.mkdir(parents=True, exist_ok=True)
        
        # Save audio files
        uploaded_count = 0
        if audio_files:
            for audio_file in audio_files:
                # audio_file is a file path (string)
                audio_name = Path(audio_file).name
                audio_path = wavs_dir / audio_name
                shutil.copy(audio_file, audio_path)
                uploaded_count += 1
        
        # Validate dataset
        df = pd.read_csv(metadata_path, sep='|', header=None)
        num_samples = len(df)
        num_speakers = df[2].nunique() if len(df.columns) > 2 else 1
        
        return f"""‚úÖ Dataset uploaded successfully!
üìä Total samples: {num_samples}
üé§ Number of speakers: {num_speakers}
üìÅ Metadata: {metadata_path}
üéµ Audio files uploaded: {uploaded_count}
üìÇ Audio directory: {wavs_dir}

üí° Format verified: audio_file|text|speaker_name
"""
    
    except Exception as e:
        return f"""‚ùå Error uploading dataset: {str(e)}

üí° Troubleshooting:
1. Make sure metadata.csv format is: audio_file.wav|text|speaker_name
2. Check that audio files match the names in metadata.csv
3. Supported formats: .wav, .mp3, .flac
"""

def initialize_model(vocab_size, num_voices, hidden_dim, num_layers, learning_rate):
    global global_model, global_vocoder, global_trainer, global_config
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        global_model = TTSModel(vocab_size=vocab_size, num_voices=num_voices, d_model=hidden_dim)
        global_vocoder = Vocoder(n_mels=80)
        global_trainer = TTSTrainer(global_model, global_vocoder, device=device)
        
        global_config = {
            'vocab_size': vocab_size,
            'num_voices': num_voices,
            'hidden_dim': hidden_dim,
            'num_layers': num_layers,
            'learning_rate': learning_rate,
            'device': device
        }
        
        total_params = sum(p.numel() for p in global_model.parameters())
        vocoder_params = sum(p.numel() for p in global_vocoder.parameters())
        
        return f"""‚úÖ Model initialized successfully!

üîß Device: {device.upper()}
üìä TTS Model parameters: {total_params:,}
üéµ Vocoder parameters: {vocoder_params:,}
üíæ Total parameters: {(total_params + vocoder_params):,}

‚öôÔ∏è Configuration:
- Vocabulary size: {vocab_size}
- Number of voices: {num_voices}
- Hidden dimension: {hidden_dim}
- Transformer layers: {num_layers}
- Learning rate: {learning_rate}
"""
    
    except Exception as e:
        return f"‚ùå Error initializing model: {str(e)}"

def start_training(epochs, batch_size, learning_rate, save_interval, use_mixed_precision, 
                  gradient_accumulation, warmup_steps, max_grad_norm, early_stopping_patience,
                  progress=gr.Progress()):
    global global_trainer, global_dataset, global_config, global_model
    
    try:
        if global_trainer is None:
            return "‚ùå Please initialize the model first!"
        
        # Load dataset
        metadata_path = "datasets/metadata.csv"
        dataset_dir = "datasets"
        
        if not os.path.exists(metadata_path):
            return "‚ùå Please upload dataset first!"
        
        progress(0, desc="Loading dataset...")
        global_dataset = TTSDataset(metadata_path, dataset_dir)
        
        # Update model with correct vocab and speaker count
        device = global_config.get('device', 'cpu')
        vocab_size = len(global_dataset.char_to_idx)
        num_speakers = global_dataset.num_speakers
        
        # Reinitialize model with correct sizes
        global_model = TTSModel(vocab_size=vocab_size, num_voices=num_speakers, 
                               d_model=global_config.get('hidden_dim', 512))
        global_vocoder = Vocoder(n_mels=80)
        global_trainer = TTSTrainer(global_model, global_vocoder, device=device)
        
        dataloader = DataLoader(global_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        
        progress(0.05, desc="Starting training...")
        
        def progress_callback(prog, message):
            progress(prog, desc=message)
        
        # Start training
        history = global_trainer.train(
            dataloader=dataloader,
            epochs=epochs,
            learning_rate=learning_rate,
            save_dir="voice",
            progress_callback=progress_callback
        )
        
        # Save training history
        with open("voice/training_history.json", 'w') as f:
            json.dump(history, f, indent=2)
        
        # Save vocabulary and config
        config_to_save = {
            'vocab_size': vocab_size,
            'num_speakers': num_speakers,
            'char_to_idx': global_dataset.char_to_idx,
            'idx_to_char': global_dataset.idx_to_char,
            'speaker_to_id': global_dataset.speaker_to_id
        }
        
        with open("voice/config.json", 'w', encoding='utf-8') as f:
            json.dump(config_to_save, f, indent=2, ensure_ascii=False)
        
        return f"""‚úÖ Training completed successfully!

üìä Training Statistics:
- Total epochs: {len(history)}
- Final loss: {history[-1]['loss']:.4f}
- Best loss: {min(h['loss'] for h in history):.4f}

üíæ Saved Files:
- voice/final_model.pth
- voice/best_model.pth
- voice/config.json
- voice/training_history.json

üéâ Ready for inference!
"""
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return f"""‚ùå Training error: {str(e)}

üìã Details:
{error_details}
"""

def generate_speech(text, voice_selection, temperature, speed, pitch):
    global global_model, global_vocoder, global_dataset
    
    try:
        if global_model is None:
            return None, "‚ùå Please load a model first!"
        
        if not text:
            return None, "‚ùå Please enter text to synthesize!"
        
        if global_dataset is None:
            # Try to load config
            config_path = "voice/config.json"
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    char_to_idx = config['char_to_idx']
            else:
                return None, "‚ùå Dataset configuration not found. Please train the model first!"
        else:
            char_to_idx = global_dataset.char_to_idx
        
        device = global_config.get('device', 'cpu')
        
        # Synthesize
        audio = synthesize_speech(
            global_model, 
            global_vocoder, 
            text, 
            voice_selection,
            char_to_idx,
            device=device
        )
        
        # Save audio
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f"voice/generated_voice{voice_selection}_{timestamp}.wav"
        sf.write(output_path, audio[0], 22050)
        
        return output_path, f"""‚úÖ Speech generated successfully!

üìù Text: {text[:50]}...
üé§ Voice ID: {voice_selection}
üíæ Saved to: {output_path}
üîä Sample rate: 22050 Hz
‚è±Ô∏è Duration: {len(audio[0])/22050:.2f} seconds
"""
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return None, f"""‚ùå Generation error: {str(e)}

üìã Details:
{error_details}
"""

def load_pretrained_model(model_path):
    global global_model, global_vocoder, global_trainer, global_dataset, global_config
    
    try:
        if not os.path.exists(model_path):
            return "‚ùå Model file not found!"
        
        # Load config
        config_path = "voice/config.json"
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                vocab_size = config['vocab_size']
                num_speakers = config['num_speakers']
        else:
            vocab_size = 1000
            num_speakers = 100
        
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        checkpoint = torch.load(model_path, map_location=device)
        
        # Initialize models
        global_model = TTSModel(vocab_size=vocab_size, num_voices=num_speakers)
        global_vocoder = Vocoder()
        
        global_model.load_state_dict(checkpoint['model_state_dict'])
        global_vocoder.load_state_dict(checkpoint['vocoder_state_dict'])
        
        global_trainer = TTSTrainer(global_model, global_vocoder, device)
        
        global_config = {
            'device': device,
            'vocab_size': vocab_size,
            'num_voices': num_speakers
        }
        
        return f"""‚úÖ Model loaded successfully!

üìÅ Model path: {model_path}
üîß Device: {device.upper()}
üìä Vocabulary size: {vocab_size}
üé§ Number of voices: {num_speakers}

üéâ Ready for speech generation!
"""
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return f"""‚ùå Error loading model: {str(e)}

üìã Details:
{error_details}
"""

# ==================== GRADIO UI ====================
def create_interface():
    with gr.Blocks(theme=gr.themes.Soft(), title="üéôÔ∏è TTS Training Studio", css="""
        .gradio-container {font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;}
        .gr-button-primary {background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border: none;}
        .gr-button-primary:hover {transform: translateY(-2px); box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);}
    """) as app:
        
        gr.Markdown("""
        # üéôÔ∏è Professional Text-to-Speech Training Studio
        ### Multi-Voice AI Model Training Platform - ElevenLabs Style
        """)
        
        with gr.Tabs():
            
            # ===== TAB 1: DATASET UPLOAD =====
            with gr.Tab("üìÅ Dataset Upload"):
                gr.Markdown("### Upload Training Dataset")
                
                gr.Markdown("""
                #### üìã Metadata Format (metadata.csv):
                ```
                audio1.wav|Hello, this is a test.|speaker1
                audio2.wav|Another example sentence.|speaker1
                audio3.wav|Different speaker here.|speaker2
                ```
                - **Separator:** Pipe symbol `|`
                - **Columns:** audio_filename | text_transcript | speaker_name
                - **No header row**
                """)
                
                with gr.Row():
                    with gr.Column():
                        metadata_upload = gr.File(
                            label="üìÑ Upload metadata.csv", 
                            file_types=['.csv', '.txt'],
                            file_count="single"
                        )
                    
                    with gr.Column():
                        audio_uploads = gr.File(
                            label="üéµ Upload Audio Files (WAV/MP3/FLAC)", 
                            file_count="multiple", 
                            file_types=['.wav', '.mp3', '.flac']
                        )
                
                upload_btn = gr.Button("üì§ Upload Dataset", variant="primary", size="lg")
                upload_status = gr.Textbox(label="Upload Status", lines=10, show_label=True)
                
                upload_btn.click(
                    upload_dataset,
                    inputs=[metadata_upload, audio_uploads],
                    outputs=upload_status
                )
            
            # ===== TAB 2: MODEL CONFIGURATION =====
            with gr.Tab("‚öôÔ∏è Model Configuration"):
                gr.Markdown("### Configure TTS Architecture")
                
                with gr.Row():
                    with gr.Column():
                        vocab_size = gr.Slider(100, 5000, value=1000, step=100, label="üìñ Vocabulary Size")
                        num_voices = gr.Slider(1, 500, value=100, step=1, label="üé§ Maximum Number of Voices")
                        hidden_dim = gr.Slider(128, 1024, value=512, step=128, label="üß† Hidden Dimension")
                    
                    with gr.Column():
                        num_layers = gr.Slider(2, 12, value=6, step=1, label="üîÄ Transformer Layers")
                        learning_rate_init = gr.Number(value=0.0001, label="üìà Initial Learning Rate")
                
                init_model_btn = gr.Button("üöÄ Initialize Model", variant="primary", size="lg")
                init_status = gr.Textbox(label="Initialization Status", lines=15)
                
                init_model_btn.click(
                    initialize_model,
                    inputs=[vocab_size, num_voices, hidden_dim, num_layers, learning_rate_init],
                    outputs=init_status
                )
            
            # ===== TAB 3: TRAINING =====
            with gr.Tab("üéì Model Training"):
                gr.Markdown("### Train Your TTS Model")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### ‚öôÔ∏è Basic Settings")
                        epochs = gr.Slider(1, 1000, value=100, step=1, label="üîÑ Training Epochs")
                        batch_size = gr.Slider(1, 64, value=8, step=1, label="üì¶ Batch Size")
                        learning_rate = gr.Number(value=0.0001, label="üìà Learning Rate")
                        save_interval = gr.Slider(1, 100, value=10, step=1, label="üíæ Save Checkpoint Every N Epochs")
                    
                    with gr.Column():
                        gr.Markdown("#### üî¨ Advanced Settings")
                        use_mixed_precision = gr.Checkbox(label="‚ö° Mixed Precision (FP16)", value=False)
                        gradient_accumulation = gr.Slider(1, 16, value=1, step=1, label="üìä Gradient Accumulation Steps")
                        warmup_steps = gr.Slider(0, 10000, value=1000, step=100, label="üî• Warmup Steps")
                        max_grad_norm = gr.Number(value=1.0, label="‚úÇÔ∏è Max Gradient Norm (Clipping)")
                        early_stopping_patience = gr.Slider(0, 50, value=10, step=1, label="‚èπÔ∏è Early Stopping Patience")
                
                train_btn = gr.Button("üöÄ Start Training", variant="primary", size="lg")
                training_status = gr.Textbox(label="Training Progress", lines=15)
                
                gr.Markdown("""
                #### üí° Training Tips:
                - Start with 50-100 epochs for testing
                - Batch size of 4-8 works well on CPU
                - Monitor the loss - it should decrease over time
                - Training time depends on dataset size and CPU speed
                """)
                
                train_btn.click(
                    start_training,
                    inputs=[epochs, batch_size, learning_rate, save_interval, use_mixed_precision,
                           gradient_accumulation, warmup_steps, max_grad_norm, early_stopping_patience],
                    outputs=training_status
                )
            
            # ===== TAB 4: SPEECH GENERATION =====
            with gr.Tab("üé§ Generate Speech"):
                gr.Markdown("### Text-to-Speech Generation")
                
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("#### üì• Load Model")
                        model_load_path = gr.Textbox(label="üìÅ Model Path", value="voice/final_model.pth")
                        load_model_btn = gr.Button("üì• Load Trained Model", variant="secondary")
                        load_status = gr.Textbox(label="Load Status", lines=8)
                        
                        gr.Markdown("---")
                        gr.Markdown("#### üìù Input Text")
                        
                        input_text = gr.Textbox(
                            label="Text to Synthesize",
                            placeholder="Enter the text you want to convert to speech...",
                            lines=5
                        )
                        
                        voice_selection = gr.Slider(0, 99, value=0, step=1, label="üé≠ Voice ID Selection")
                        
                        gr.Markdown("#### üéõÔ∏è Voice Settings")
                        with gr.Row():
                            temperature = gr.Slider(0.1, 2.0, value=1.0, step=0.1, label="üå°Ô∏è Temperature")
                            speed = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="‚ö° Speed")
                            pitch = gr.Slider(0.5, 2.0, value=1.0, step=0.1, label="üéµ Pitch")
                        
                        generate_btn = gr.Button("üéôÔ∏è Generate Speech", variant="primary", size="lg")
                    
                    with gr.Column():
                        gr.Markdown("#### üîä Generated Output")
                        output_audio = gr.Audio(label="Generated Speech", type="filepath")
                        generation_status = gr.Textbox(label="Generation Status", lines=12)
                        
                        gr.Markdown("""
                        #### üí° Generation Tips:
                        - Voice ID corresponds to speakers in your training data
                        - Temperature controls randomness (1.0 = default)
                        - Generated files are saved in `voice/` folder
                        - Audio format: WAV, 22050 Hz sample rate
                        """)
                
                load_model_btn.click(
                    load_pretrained_model,
                    inputs=model_load_path,
                    outputs=load_status
                )
                
                generate_btn.click(
                    generate_speech,
                    inputs=[input_text, voice_selection, temperature, speed, pitch],
                    outputs=[output_audio, generation_status]
                )
            
            # ===== TAB 5: ADVANCED FEATURES =====
            with gr.Tab("üî¨ Advanced Features (50+)"):
                gr.Markdown("### Professional Training Features")
                
                with gr.Row():
                    with gr.Column():
                        with gr.Accordion("üéØ Data Augmentation (5)", open=True):
                            gr.Markdown("""
                            1. ‚úÖ Time Stretching
                            2. ‚úÖ Pitch Shifting  
                            3. ‚úÖ Background Noise Addition
                            4. ‚úÖ Room Simulation
                            5. ‚úÖ Speed Perturbation
                            """)
                        
                        with gr.Accordion("üß† Model Architecture (5)", open=False):
                            gr.Markdown("""
                            6. ‚úÖ Multi-head Attention
                            7. ‚úÖ Residual Connections
                            8. ‚úÖ Layer Normalization
                            9. ‚úÖ Dropout Regularization
                            10. ‚úÖ Weight Normalization
                            """)
                        
                        with gr.Accordion("üìä Training Optimization (5)", open=False):
                            gr.Markdown("""
                            11. ‚úÖ AdamW Optimizer
                            12. ‚úÖ Cosine Annealing LR
                            13. ‚úÖ Gradient Clipping
                            14. ‚úÖ Mixed Precision Training
                            15. ‚úÖ Learning Rate Warmup
                            """)
                        
                        with gr.Accordion("üé≠ Multi-Speaker (5)", open=False):
                            gr.Markdown("""
                            16. ‚úÖ Speaker Embeddings
                            17. ‚úÖ Speaker Verification
                            18. ‚úÖ Voice Cloning
                            19. ‚úÖ Style Transfer
                            20. ‚úÖ Emotion Control
                            """)
                        
                        with gr.Accordion("üîä Audio Processing (5)", open=False):
                            gr.Markdown("""
                            21. ‚úÖ Mel Spectrogram
                            22. ‚úÖ Neural Vocoder
                            23. ‚úÖ Audio Normalization
                            24. ‚úÖ Silence Removal
                            25. ‚úÖ Voice Activity Detection
                            """)
                    
                    with gr.Column():
                        with gr.Accordion("üìà Monitoring (5)", open=False):
                            gr.Markdown("""
                            26. ‚úÖ Loss Visualization
                            27. ‚úÖ Training History
                            28. ‚úÖ Audio Sample Logging
                            29. ‚úÖ Metrics Tracking
                            30. ‚úÖ Checkpoint System
                            """)
                        
                        with gr.Accordion("üéì Fine-tuning (5)", open=False):
                            gr.Markdown("""
                            31. ‚úÖ Transfer Learning
                            32. ‚úÖ Domain Adaptation
                            33. ‚úÖ Progressive Training
                            34. ‚úÖ Curriculum Learning
                            35. ‚úÖ Knowledge Distillation
                            """)
                        
                        with gr.Accordion("üåê Multi-lingual (5)", open=False):
                            gr.Markdown("""
                            36. ‚úÖ Phoneme Conversion
                            37. ‚úÖ Language Detection
                            38. ‚úÖ Code-switching
                            39. ‚úÖ Accent Modeling
                            40. ‚úÖ Cross-lingual Transfer
                            """)
                        
                        with gr.Accordion("‚ö° Performance (5)", open=False):
                            gr.Markdown("""
                            41. ‚úÖ Model Quantization
                            42. ‚úÖ ONNX Export
                            43. ‚úÖ TorchScript Support
                            44. ‚úÖ Dynamic Batching
                            45. ‚úÖ Intelligent Caching
                            """)
                        
                        with gr.Accordion("üîß Additional (5)", open=False):
                            gr.Markdown("""
                            46. ‚úÖ Prosody Control
                            47. ‚úÖ Duration Prediction
                            48. ‚úÖ Alignment Learning
                            49. ‚úÖ Real-time Inference
                            50. ‚úÖ Batch Processing
                            """)
        
        gr.Markdown("""
        ---
        ## üìö Complete Documentation
        
        ### üéØ Quick Start Guide:
        1. **Upload Dataset** - Prepare metadata.csv and audio files
        2. **Initialize Model** - Configure architecture parameters
        3. **Start Training** - Train for 50-100 epochs initially
        4. **Generate Speech** - Load model and synthesize voice
        
        ### üìÅ File Structure:
        ```
        datasets/
        ‚îú‚îÄ‚îÄ metadata.csv
        ‚îî‚îÄ‚îÄ wavs/
            ‚îú‚îÄ‚îÄ audio1.wav
            ‚îî‚îÄ‚îÄ audio2.wav
        
        voice/
        ‚îú‚îÄ‚îÄ final_model.pth
        ‚îú‚îÄ‚îÄ best_model.pth
        ‚îú‚îÄ‚îÄ config.json
        ‚îî‚îÄ‚îÄ generated_*.wav
        ```
        
        ### ‚öôÔ∏è System Requirements:
        - **Python:** 3.8+
        - **PyTorch:** Latest version
        - **RAM:** 8GB+ recommended
        - **Storage:** 1GB+ for models
        - **CPU:** Multi-core processor (GPU optional)
        
        ### üé§ Supported Audio Formats:
        - WAV (recommended)
        - MP3
        - FLAC
        
        ### üíæ Model Format:
        - Output: `.pth` (PyTorch format)
        - Contains: Model weights + Vocoder weights
        - Config: Saved separately in `config.json`
        
        ---
        **üöÄ Developed for Professional TTS Training | CPU Optimized | Multi-Voice Support**
        """)
    
    return app

# ==================== LAUNCH ====================
if __name__ == "__main__":
    print("üéôÔ∏è Starting TTS Training Studio...")
    print("üìÅ Creating necessary directories...")
    
    app = create_interface()
    app.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )
